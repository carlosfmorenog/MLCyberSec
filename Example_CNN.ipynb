{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SSCxZ_rnDMU"
      },
      "source": [
        "# Image Classification using (Convolutional) Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAByEdJgnDMZ"
      },
      "source": [
        "Now that you know the basis of a Machine Learning, you will run one of them in very few lines of code! To do so, we will use `Keras` with a `Tensorflow` backend, along with the very popular `mnist` dataset of handwritten numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfNlq1WknDMc"
      },
      "source": [
        "First, install the necessary packages if you don't have them already:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9grvxB__nDMf"
      },
      "outputs": [],
      "source": [
        "# 0. Installing the necesssary packages\n",
        "!pip install keras\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VUSmyOSnDMk"
      },
      "source": [
        "Then we will ensure that `Keras` uses `Tensorflow` as backend. Notice that if you prefer to use another backend such as `Theano`, you simply need to change the name in the second line of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA00Jy9CnDMm"
      },
      "outputs": [],
      "source": [
        "# 1. Ensure you are using Theano backend\n",
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqaLJp47nDMn"
      },
      "source": [
        "Now you will import the necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkZUAk8NnDMo"
      },
      "outputs": [],
      "source": [
        "# 2. Import libraries and modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOcoU4RynDMq"
      },
      "source": [
        "Then, we will set a **random seed** to be able to repeat the results and get the same results every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0eUxbH6nDMr"
      },
      "outputs": [],
      "source": [
        "# 3. Set random seed (for reproducibility)\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-xSJAVJnDMs"
      },
      "source": [
        "With the following cell you will download the data from the `mnist` dataset. Notice that the data comes already partitioned in test and training sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04S0QwfnnDMt"
      },
      "outputs": [],
      "source": [
        "# 4. Load pre-shuffled MNIST data into train and test sets\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9zkF8UMnDMu"
      },
      "source": [
        "Let's check the shape of the things obtained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDm-Hn2YnDMv"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OCbcXrwnDMw"
      },
      "source": [
        "Notice that we have 60'000 samples for training and 10'000 for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBkxkbCmnDMw"
      },
      "source": [
        "Now we will preprocess the data to be used by the classifier. To be able to use Keras, we need to do the following:\n",
        "1. **Reshape** the data into **four** dimensions i.e. the training set will be of shape (60000,28,28,1) and the test set of (10000,28,28). This is useful since the network needs an input shape of (1,28,28) **for each of the samples**.\n",
        "2. Convert the format of the input into `float32` (apparently the CNN works better with it).\n",
        "3. **Normalise** i.e. divide all values by 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tj9YxpYfnDMx"
      },
      "outputs": [],
      "source": [
        "# 5. Preprocess input data\n",
        "# Reshape into four dimensions.\n",
        "X_train_reshape = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test_reshape = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "# Convert to float 32\n",
        "X_train_reshape = X_train_reshape.astype('float32')\n",
        "X_test_reshape = X_test_reshape.astype('float32')\n",
        "# normalise\n",
        "X_train_reshape /= 255 \n",
        "X_test_reshape /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSOyV4UbnDM0"
      },
      "source": [
        "CNNs also like their target to be categorical, i.e. instead of the target being values from 0 to 9, each target value will be a vector indicating which is the class according to the position. Run the following cell to see what I mean..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7QcNnlfnDM1"
      },
      "outputs": [],
      "source": [
        "# 6. Preprocess class labels\n",
        "Y_train_categorical = np_utils.to_categorical(Y_train, 10)\n",
        "Y_test_categorical = np_utils.to_categorical(Y_test, 10)\n",
        "# Show a sample target entry. You will see that this sample corresponds to a 5 as\n",
        "# there is a five in the 0th position (remember that python starts in 0)\n",
        "print(Y_train_categorical[0])\n",
        "plt.imshow(X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgx8rjqdnDM2"
      },
      "source": [
        "Now it's time to train the model. We will define a **sequential** CNN with two convolutional layers, a **max pooling** of size $2 \\times 2$ and a **dropout** of $0.25$. Then, we will add a **flatten** layer, add a **densely connected** layer with a **ReLu** activation, afterwards add another **dropout** of $0.5$, and finally add a densely connected layer to the output with a **softmax** activation function. This configuration is not strict and you can find many different examples, such as [this other one](https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYtZp2H6nDM3"
      },
      "outputs": [],
      "source": [
        "# 7. Define model architecture\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "\n",
        "model = Sequential()\n",
        " \n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To be congruent with what we saw on the lecture, let's see the model summary. Notice that before the dropout layer we had 589k features! But afterwards only 1290 are used."
      ],
      "metadata": {
        "id": "WcubYnCFSQS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Print a summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CMn9aBgbSQpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irGWXAn0nDM4"
      },
      "source": [
        "After creating the model architecture, we will compile it. We will use the **ADAM** optimiser to improve the loss obtained by the **categorical cross entropy** method, and then we will request the model to obtain the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPRq-xp8nDM5"
      },
      "outputs": [],
      "source": [
        "# 9. Compile model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVlqDQ1PnDM5"
      },
      "source": [
        "Once compiled, we will fit this model using our training data. If your computer is slow, I recommend you **NOT** to use the entire training dataset. This can be done by reducing the number `n` to something smaller than 60'000. Also, you can reduce the number of **epochs**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta32g1OAnDM6"
      },
      "outputs": [],
      "source": [
        "# 10. Fit model on training data\n",
        "n=6000 # In google colab, don't worry if you use all the data!\n",
        "\n",
        "model.fit(X_train_reshape[:n], Y_train_categorical[:n], \n",
        "          batch_size=32, epochs=5, verbose=1) # verbose = 1 lets you see the training log for each iteration, higher values just gives you a summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBBg9ykGnDM8"
      },
      "source": [
        "We can try to add more `n` and epochs to see how much `training accuracy` we are capable to achieve. Also, keep in mind that if the `loss` is not going down, then the model is not learning! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmtWTbg1nDM8"
      },
      "source": [
        "Now we can evaluate your model in the `test` data. We can as well obtain the `loss` and the `accuracy` for this new, unseen data for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEFP8jRfnDM9"
      },
      "outputs": [],
      "source": [
        "# 11. Evaluate model on test data\n",
        "loss, accuracy = model.evaluate(X_test_reshape[:n], Y_test_categorical[:n], verbose=0)\n",
        "print('Loss: ', loss,'\\nAcc: ', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU40tKiqnDM9"
      },
      "source": [
        "With the following cell you can print the labels that were predicted by the model. Notice that the classes are not categorical anymore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4ThHn12nDM-"
      },
      "outputs": [],
      "source": [
        "# 12. Check the labels that have been predicted\n",
        "predict_x=model.predict(X_test[:n]) \n",
        "classes_x=np.argmax(predict_x,axis=1)\n",
        "print(classes_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkjLgh9ZnDM-"
      },
      "source": [
        "The next cell has a brief code that will help you find the incorrect labels by comparing the labels obtained for the test samples with their ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-C-VpWPnDM_"
      },
      "outputs": [],
      "source": [
        "# 13. Check the label that has been predicted incorrectly\n",
        "incorrect_labels=[]\n",
        "accuracy = 0\n",
        "for i,cla in enumerate(classes_x):\n",
        "  if cla != Y_test[:n][i]:\n",
        "    print(\"Sample \"+str(i)+\" was classified as \"+str(cla)+\" when it really was \"+str(Y_test[:n][i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1oAQn11nDNA"
      },
      "source": [
        "Finally, you can use the following cell to print training or test cells from the dataset and see their actual (and predicted, for the case of test) labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78oido3SnDNA"
      },
      "outputs": [],
      "source": [
        "# 14. Show a sample from the mnist dataset\n",
        "image_to_show = 72\n",
        "from_group = 'test' # 'train' or 'test'\n",
        "if from_group == 'train':\n",
        "    plt.imshow(X_train[image_to_show])\n",
        "    print('Ground truth label: ',Y_train[image_to_show])\n",
        "else:\n",
        "    plt.imshow(X_test[image_to_show])\n",
        "    print('Ground truth label: ',Y_test[image_to_show])\n",
        "    if len(predict_x)>image_to_show:\n",
        "        print('Predicted label: ',classes_x[image_to_show])"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}