{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RVZPsy7-0K7"
      },
      "source": [
        "# Face Detection in Python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning the GitHub repository\n",
        "!git clone https://github.com/carlosfmorenog/MLCyberSecBiometrics"
      ],
      "metadata": {
        "id": "6x6RDzBuoxzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentals of face detection"
      ],
      "metadata": {
        "id": "s7vn9XNIqUji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0oRjLFw-0K8"
      },
      "source": [
        "The human face innately has features (most of the times)!\n",
        "- nose\n",
        "- eyes\n",
        "- ears\n",
        "- lips\n",
        "- etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Oq-jtX0-0K8"
      },
      "source": [
        "As humans, we use those features to recognise other individuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIvrM1KX-0K8"
      },
      "source": [
        "Then why wouldn't a machine use them as well?!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wnf_jl6-0K8"
      },
      "source": [
        "One of the first and most famous frameworks for face detection was presented in 2001 by Paul Viola and Michael Jones, often referred to as the [Viola-Jones](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework) object detection framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qTC5EU1-0K8"
      },
      "source": [
        "It uses something called [Haar-like features](https://realpython.com/traditional-face-detection-python/) to detect edges (1 and 2), lines (3) and diagonals (4)\n",
        "![Fig. 7. Haar-like features](https://www.dropbox.com/s/hijnqhik9jll6l3/haar.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VPTxP84-0K9"
      },
      "source": [
        "It turns out that when overlapping these features in a human face, it can help us detect areas of interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21JV2MMV-0K9"
      },
      "source": [
        "For example, *mask 2* can help us identify the area with the eyes\n",
        "![Fig. 8. Eye detection](https://www.dropbox.com/s/u70jfji3h9svq0k/haareyes.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_jyLEYx-0K9"
      },
      "source": [
        "Conversely, *mask 3* can help us find the nose\n",
        "![Fig. 9. Nose detection](https://www.dropbox.com/s/z70sno7xfckidat/haarnose.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkIKS08e-0K-"
      },
      "source": [
        "A simple classifier would right away deduct that, if there are eyes and nose, then there must be a face!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtwNNgwH-0K-"
      },
      "source": [
        "This is the algorithm most commonly used in commercial cameras (it is fast and easy to use)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHJthJeK-0K-"
      },
      "source": [
        "This also explains why faces in statues and paintings get detected!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEc3i_1L-0LA"
      },
      "source": [
        "It would take me another 5 hours to explain all the details of this method, the only thing you need to know is that it is an **end-to-end approach**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that authors not only proposed the feature extraction, but also the classifier!"
      ],
      "metadata": {
        "id": "tl5cfJwLoqTF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ppf5Qq-0LA"
      },
      "source": [
        "In this case, they use a **cascading classifier** (you will sometimes find this in literature as Haar cascade)\n",
        "![Fig. 10. Basics of a cascade classifier](https://www.dropbox.com/s/z98smtkkndyk0yq/cascade.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk9qRfPu-0LA"
      },
      "source": [
        "This can get more complex (but also more robust) as more classifiers are used\n",
        "![Fig. 11. Full cascade classifier](https://www.dropbox.com/s/ndz2mvq3xp09jol/haarcascade.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUGT4Dld-0LB"
      },
      "source": [
        "There is a package called `OpenCV` in Python, which comes with a ready to use Haar cascade!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing the model"
      ],
      "metadata": {
        "id": "CvDGx6SVqYfM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtfV_2Sk-0LB"
      },
      "source": [
        "First, we need to import an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9MpK2Gq-0LB"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Read image\n",
        "original_image = cv2.imread('/content/MLCyberSecFace/nomask.jpg')\n",
        "# show converted because cv uses BGR, not RGB\n",
        "plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9an3xR5-0LB"
      },
      "source": [
        "Then, we convert the image to grayscale so that we can apply the Viola-Jones method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9utDk304-0LB"
      },
      "outputs": [],
      "source": [
        "# Convert color image to grayscale for Viola-Jones\n",
        "grayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "# We use \"gray\" to specify to plt.imshow that the image is grayscale\n",
        "plt.imshow(grayscale_image, 'gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdR_gC13-0LB"
      },
      "source": [
        "Then, we use the **frontalface** .xml model (which someone has already created for us). Load it here and use it to process the image using the `detectMultiScale()` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tOPqFmL-0LB"
      },
      "outputs": [],
      "source": [
        "# Load the classifier and create a cascade object for face detection\n",
        "face_cascade = cv2.CascadeClassifier('/content/MLCyberSecFace/haarcascade_frontalface_alt.xml')\n",
        "# Detect faces\n",
        "detected_faces = face_cascade.detectMultiScale(grayscale_image)\n",
        "print(detected_faces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdwEwx-6-0LC"
      },
      "source": [
        "Notice that we get four numbers! These are the coordinates where the face is found."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RMzHn8--0LC"
      },
      "source": [
        "We can do a method to \"draw\" green rectangles over the faces as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzw9n_lj-0LC"
      },
      "outputs": [],
      "source": [
        "# Put rectangles in the images\n",
        "for (column, row, width, height) in detected_faces:\n",
        "    cv2.rectangle(original_image,(column, row),\n",
        "        (column + width, row + height),\n",
        "        (0, 255, 0),2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpH_5q4T-0LC"
      },
      "source": [
        "Finally, we show the original image with the rectangle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Giegjy0M-0LC"
      },
      "outputs": [],
      "source": [
        "plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Qr3Q8v-0LD"
      },
      "source": [
        "Do you think this works with a facemask?\n",
        "![Fig. 12. Facemask example 1](https://www.dropbox.com/s/hu4idfcm07xk528/facemask.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6MnuQGf-0LD"
      },
      "outputs": [],
      "source": [
        "original_image = cv2.imread('/content/MLCyberSecFace/facemask.jpg')\n",
        "grayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "face_cascade = cv2.CascadeClassifier('/content/MLCyberSecFace/haarcascade_frontalface_alt.xml')\n",
        "detected_faces = face_cascade.detectMultiScale(grayscale_image)\n",
        "print(detected_faces)\n",
        "for (column, row, width, height) in detected_faces:\n",
        "    cv2.rectangle(original_image,(column, row),\n",
        "        (column + width, row + height),\n",
        "        (0, 255, 0),2)\n",
        "plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C_ULdKW-0LD"
      },
      "source": [
        "How about now?\n",
        "![Fig. 13. Facemask example 2](https://www.dropbox.com/s/594o736z80aauxm/facemask2.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKYajMnc-0LE"
      },
      "outputs": [],
      "source": [
        "original_image = cv2.imread('/content/MLCyberSecFace/facemask2.jpg')\n",
        "grayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n",
        "face_cascade = cv2.CascadeClassifier('/content/MLCyberSecFace/haarcascade_frontalface_alt.xml')\n",
        "detected_faces = face_cascade.detectMultiScale(grayscale_image)\n",
        "print(detected_faces)\n",
        "for (column, row, width, height) in detected_faces:\n",
        "    cv2.rectangle(original_image,(column, row),\n",
        "        (column + width, row + height),\n",
        "        (0, 255, 0),2)\n",
        "plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BONUS**: Do you think there is anything that can be done for this image?"
      ],
      "metadata": {
        "id": "W-dOZkJmrJen"
      }
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "rise": {
      "enable_chalkboard": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}